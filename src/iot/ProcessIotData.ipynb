{
  "metadata": {
    "saveOutput": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "#This notebook is inteded for demo puposes.\n",
        "#Scenario:\n",
        "#An IOT device is sending data to a Kafka endpoint. This notebook consumes the data and performs the following functions:\n",
        "#1. Store data in a folder partitioned format on cloud storage by deviceName,tagName, year, month, day, hour\n",
        "#2. Ensure there is not a duplicate of the specific value stored in the \"value\" column to be sent to downstream databases. (This looks at a specific value not duplicate for a row of data.)\n",
        "#   The objective is to only capture changes in the \"value\" column. Value can only be a 0 or 1.\n",
        "\n",
        "#To accomplish point to the following logic is done:\n",
        "#   Take data from the Kafka endpoint and store data in delta table for further processing. The requirement for storing the data \n",
        "#   is to check if the data is within the last seven days. If it meets this requirement, it will attempt to place the data in table by \n",
        "#   checking the values of the entries that occured before and after the timestamp of the consumed data from kafka. If there is a value\n",
        "#   already entered we will ignore the new entry. \n",
        "#   \n",
        "#   Once data is stored we will send to downstream SQL database for reporting.\n",
        "    \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "#Create a function to handle writes. Data is persisted before write to prevent recomputation of data.\n",
        "def writeData(df):\n",
        "    #Set up SQL connection information to use Spark SQL Connector\n",
        "    server_name = \"jdbc:sqlserver://{SERVER_ADDR}\"\n",
        "    database_name = \"database_name\"\n",
        "    url = server_name + \";\" + \"databaseName=\" + database_name + \";\"\n",
        "\n",
        "    table_name = \"table_name\"\n",
        "    username = \"username\"\n",
        "    password = \"password123!#\" # Please specify password here\n",
        "\n",
        "    df.persist()\n",
        "    df.write.format(\"delta\").mode(\"append\").partitionBy('day').save(\"/data/silver\")\n",
        "    df.write \\\n",
        "        .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
        "        .mode(\"append\") \\\n",
        "        .option(\"url\", url) \\\n",
        "        .option(\"dbtable\", table_name) \\\n",
        "        .option(\"user\", username) \\\n",
        "        .option(\"password\", password) \\\n",
        "        .save()\n",
        "    df.unpersist()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from datetime import timezone\n",
        "from delta.tables import *\n",
        "import datetime\n",
        "\n",
        "#Created function to be used in foreachbactch\n",
        "def processRow(df, batchId):\n",
        "        #Create dataframe to be used to insert to SQL, need to cast year,month,day,hour as integer\n",
        "        newDF = df.withColumn(\"year\", from_unixtime(col(\"time\"), 'yyyy').cast(IntegerType())) \\\n",
        "            .withColumn(\"month\", from_unixtime(col(\"time\"), 'MM').cast(IntegerType())) \\\n",
        "            .withColumn(\"day\", from_unixtime(col(\"time\"), 'dd').cast(IntegerType())) \\\n",
        "            .withColumn(\"hour\", from_unixtime(col(\"time\"), 'HH').cast(IntegerType()))\n",
        "\n",
        "\n",
        "        #Get unix timestap from seven days ago at the current datetime.\n",
        "        unixAWeekAgo = ((datetime.datetime.now()) - (datetime.timedelta(days=7))).replace(tzinfo=timezone.utc).timestamp()\n",
        "        #Get timestamp from data received\n",
        "        time = newDF.toPandas().iat[0][2]\n",
        "\n",
        "\n",
        "\n",
        "        #Ensure time is greater than 7 days ago\n",
        "        if(time>unixAWeekAgo):\n",
        "            #Verify if Delta table has been created, if not create table and insert data.\n",
        "            if(DeltaTable.isDeltaTable(spark,\"/data/silver\")== False):\n",
        "                #persist data to prevent recomputation at each write.\n",
        "                newDf.persist()\n",
        "                newDF.write.format(\"delta\").partitionBy(\"day\").option(\"path\",\"/data/silver\").saveAsTable(\"events\")\n",
        "                #If using Spark 2.4-see below\n",
        "                #newDF.write.format(\"delta\").mode(\"append\").partitionBy('day') \\\n",
        "                #    .option(\"__partition_columns\", \"\"\"[\"day\"]\"\"\").option(\"path\",\"/data/silver\") \\\n",
        "                #    .saveAsTable(\"events\")\n",
        "\n",
        "                #Write to SQL\n",
        "                newDF.write \\\n",
        "                    .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
        "                    .mode(\"append\") \\\n",
        "                    .option(\"url\", url) \\\n",
        "                    .option(\"dbtable\", table_name) \\\n",
        "                    .option(\"user\", username) \\\n",
        "                    .option(\"password\", password) \\\n",
        "                    .save()\n",
        "                newDF.unpersist()\n",
        "            else: \n",
        "                #Get day and hour values       \n",
        "                day = newDF.toPandas().iat[0][6]\n",
        "                hour = newDF.toPandas().iat[0][7]\n",
        "                #Get the result that happend prior to this timestamp\n",
        "                lastResult = spark.sql(\"SELECT * FROM events WHERE day = {0} AND hour = {1} AND time < {2} ORDER BY time DESC limit 1\".format(day,hour,time))\n",
        "                #Get the result that happend after this timestamp\n",
        "                afterResult = spark.sql(\"SELECT * FROM events WHERE day = {0} AND hour = {1} AND time > {2} ORDER BY time DESC limit 1\".format(day,hour,time))\n",
        "            \n",
        "            #Get values from the new data and the data for the last and after result.\n",
        "                rawValue = newDF.toPandas().iat[0][3]\n",
        "                lastResultValue = lastResult.toPandas().iat[0][3]\n",
        "                afterResultValue = afterResult.toPandas().iat[0][3]\n",
        "\n",
        "                #Compare results to see if a insert is needed.\n",
        "                #There is a result for before and after\n",
        "                if(lastResult.count() == 1 and afterResult.count() == 1):\n",
        "                    #Check if the previous result and the after result are not the same as the dataframe current result\n",
        "                    if (rawValue != lastResultValue and rawValue != afterResultValue):\n",
        "                        writeData(df)\n",
        "                #Check if no previous result but there was a result after\n",
        "                elif(lastResult.count()== 0 and afterResult.count() == 1):\n",
        "                    #Verify that the after result is not equal to the current dataframe\n",
        "                    if(rawValue != afterResultValue):        \n",
        "                        writeData(df)\n",
        "                #Check if there is a pervious result but not one after.\n",
        "                elif(lastResult.count()==1 and afterResult.count() == 0):\n",
        "                    #Verify that the last result does not match the current dataframe result.\n",
        "                    if(rawValue != lastResultValue):\n",
        "                        writeData(df)\n",
        "                else:\n",
        "                    #There is no data in table, inserting.\n",
        "                    writeData(df)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# define Schema for reading data off kafka topic\n",
        "schema = StructType().add(\"deviceName\", StringType()).add(\"tagName\", StringType()) \\\n",
        "    .add(\"time\", StringType()).add(\"value\", StringType())"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "#Read data from kafka topic\n",
        "df = spark \\\n",
        "  .readStream \\\n",
        "  .format(\"kafka\") \\\n",
        "  .option(\"kafka.bootstrap.servers\", \"127.0.0.1:9092\") \\\n",
        "  .option(\"subscribe\", \"iot-events\") \\\n",
        "  .load() \\\n",
        "  .select(from_json(col(\"value\").cast(\"string\"), schema))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import from_unixtime , col\n",
        "\n",
        "#Add columns for folder partitioning, date is stored in unix time\n",
        "storageDF = df.withColumn(\"year\", from_unixtime(col(\"time\"), 'yyyy')) \\\n",
        "    .withColumn(\"month\", from_unixtime(col(\"time\"), 'MM')) \\\n",
        "    .withColumn(\"day\", from_unixtime(col(\"time\"), 'dd')) \\\n",
        "    .withColumn(\"hour\", from_unixtime(col(\"time\"), 'HH'))\n",
        "\n",
        "#Retrieve value for each (row,column) for path creation of storing data.\n",
        "deviceName = df.toPandas().iat[0][0]\n",
        "tagName = df.toPandas().iat[0][1]\n",
        "year = storageDF.toPandas().iat[0][4]\n",
        "month = storageDF.toPandas().iat[0][5]\n",
        "day = storageDF.toPandas().iat[0][6]\n",
        "hour = storageDF.toPandas().iat[0][7]\n",
        "\n",
        "#Create Path\n",
        "storage_path = '/Data/%s/%s/%s/%s/%s/%s' % (deviceName, tagName, year,month,day,hour)\n",
        "\n",
        "#Write Raw data to folder.\n",
        "storageDF.write.parquet(storage_path, mode = \"append\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "#use foreachbatch funcationality to auto process each df.\n",
        "df.writeStream \\\n",
        "    .foreachBatch(processRow) \\ \n",
        "    .start()"
      ],
      "attachments": {}
    }
  ]
}